{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910f88b9-bb03-4746-adc6-98849d634837",
   "metadata": {
    "id": "910f88b9-bb03-4746-adc6-98849d634837"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from timm import create_model  # pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "X5hzdj9-_rQE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5hzdj9-_rQE",
    "outputId": "edad4fab-06d1-4bb2-e8c2-7b1e21514606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset unzipped into datasets/preprocessed/\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = \"/content/preprocessed.zip\"\n",
    "extract_path = \"datasets/preprocessed\"\n",
    "\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"Dataset unzipped into datasets/preprocessed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe16168-d0d3-4707-afbe-3769f03ac898",
   "metadata": {},
   "source": [
    "## Swin Transformer on Pre-processed Fundus Images\n",
    "\n",
    "- Using `timm.create_model('swin_tiny_patch4_window7_224', pretrained=True)`  \n",
    "- Output layer modified for **2 classes**: `No_DR` and `DR`.\n",
    "- Model is moved to **GPU if available**, else CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6074a8c-cfd2-4fed-a74d-64380abe27ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6074a8c-cfd2-4fed-a74d-64380abe27ca",
    "outputId": "2c677577-efc8-43d9-a5f6-21454edf3034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435ed8fb-6203-49cc-8147-181edd19e59f",
   "metadata": {
    "id": "435ed8fb-6203-49cc-8147-181edd19e59f"
   },
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "class DRDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        label_map = {\"No_DR\": 0, \"DR\": 1}\n",
    "\n",
    "        for label in os.listdir(root_dir):\n",
    "            if label not in label_map:\n",
    "                continue\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            for img_file in os.listdir(label_dir):\n",
    "                self.images.append(os.path.join(label_dir, img_file))\n",
    "                self.labels.append(label_map[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474c17c7-ae8a-47e7-bf41-bd3477d1086a",
   "metadata": {
    "id": "474c17c7-ae8a-47e7-bf41-bd3477d1086a"
   },
   "outputs": [],
   "source": [
    "# Transforms\n",
    "IMG_SIZE = 224  # Swin Transformer expects 224x224\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b182acdd-0d2a-4d63-8a2b-6a52be447de6",
   "metadata": {
    "id": "b182acdd-0d2a-4d63-8a2b-6a52be447de6"
   },
   "outputs": [],
   "source": [
    "# Datasets & DataLoaders\n",
    "\n",
    "train_dir = \"/content/datasets/preprocessed/preprocessed/train\"\n",
    "val_dir   = \"/content/datasets/preprocessed/preprocessed/val\"\n",
    "test_dir  = \"/content/datasets/preprocessed/preprocessed/test\"\n",
    "\n",
    "train_dataset = DRDataset(train_dir, transform=train_transform)\n",
    "val_dataset   = DRDataset(val_dir, transform=val_test_transform)\n",
    "test_dataset  = DRDataset(test_dir, transform=val_test_transform)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce53cb0d-4572-4009-921c-70b41130e015",
   "metadata": {
    "id": "ce53cb0d-4572-4009-921c-70b41130e015"
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "# Create Swin Transformer (tiny) pretrained on ImageNet\n",
    "model = create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=2)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc985cc5-1cf1-4461-947d-d795252b8a65",
   "metadata": {
    "id": "bc985cc5-1cf1-4461-947d-d795252b8a65"
   },
   "outputs": [],
   "source": [
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c12aa09-49e5-4e52-8b05-382ef80b7f69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c12aa09-49e5-4e52-8b05-382ef80b7f69",
    "outputId": "b54a1805-c410-4225-d929-6a3eaa5408ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 633/633 [00:38<00:00, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Training Loss: 0.8276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Validation Accuracy: 0.5367\n",
      " Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 633/633 [00:38<00:00, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Training Loss: 0.8266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 633/633 [00:38<00:00, 16.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Training Loss: 0.8272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 633/633 [00:38<00:00, 16.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Training Loss: 0.8257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 633/633 [00:38<00:00, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Training Loss: 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 633/633 [00:38<00:00, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Training Loss: 0.8254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 633/633 [00:38<00:00, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Training Loss: 0.8266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 633/633 [00:38<00:00, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Training Loss: 0.8264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 633/633 [00:39<00:00, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Training Loss: 0.8276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Validation Accuracy: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 633/633 [00:38<00:00, 16.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Training Loss: 0.8279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Validation Accuracy: 0.5367\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "NUM_EPOCHS = 10\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    #  Validation \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save Best Model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_swin_transformer.pth\")\n",
    "        print(\" Saved new best model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3afc17-61fb-4afc-b8c1-ee366c65db40",
   "metadata": {},
   "source": [
    "###  Final Evaluation\n",
    "We evaluate the enhanced model on the **test dataset**, giving the final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6509124f-27fd-4539-a6fc-73f80eb1e4b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6509124f-27fd-4539-a6fc-73f80eb1e4b6",
    "outputId": "0bc4f016-5f31-45a8-a763-62a90dda669a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TEST ACCURACY: 0.5365168539325843\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/content/best_swin_transformer.pth\"))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(\" TEST ACCURACY:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjoHRtO0K2w3",
   "metadata": {
    "id": "yjoHRtO0K2w3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DR-Vision Env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
